{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48db32f",
   "metadata": {},
   "source": [
    "# PyBADS Example 5: Extended usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b5d8b",
   "metadata": {},
   "source": [
    "In this example, we will show PyBADS at work on a target with multiple local minima (also referred to as \"multimodal\" in statistics), and showcase some additional features of the package.\n",
    "\n",
    "This notebook is Part 5 of a series of notebooks in which we present various example usages for BADS with the PyBADS package.\n",
    "The code used in this example is available as a script [here](./scripts/pybads_example_5_extended_usage.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c7306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pybads import BADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd1931",
   "metadata": {},
   "source": [
    "## 1. Problem setup\n",
    "\n",
    "In this example, we are going to optimize the *six-hump camelback function*, which has six local minima, two of which are global minima.\n",
    "\n",
    "Note that, in most realistic scenarios, you would not know whether your problem has only a single local minimum (which is also the global minimum). In practice, many optimization problems exhibit *multiple* local minima and you should assume so, in the absence of additional knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192fbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camelback6(x):\n",
    "    \"\"\"Six-hump camelback function.\"\"\"\n",
    "    x_2d = np.atleast_2d(x)\n",
    "    x1 = x_2d[:,0]\n",
    "    x2 = x_2d[:,1]\n",
    "    f = (4 - 2.1*(x1*x1) + (x1*x1*x1*x1)/3.0)*(x1*x1) + x1*x2 + (-4 + 4*(x2*x2))*(x2*x2)\n",
    "    return f\n",
    "\n",
    "lower_bounds = np.array([-3, -2])\n",
    "upper_bounds = np.array([3, 2])\n",
    "plausible_lower_bounds = np.array([-2.9, -1.9])\n",
    "plausible_upper_bounds = np.array([2.9, 1.9])\n",
    "\n",
    "options = {\n",
    "    \"display\" : 'off',             # We switch off the printing\n",
    "    \"uncertainty_handling\": False, # Good to specify that this is a deterministic function\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e280e",
   "metadata": {},
   "source": [
    "## 2. Run the optimization\n",
    "\n",
    "PyBADS is **not** a global optimization algorithm in that there is no guarantee that a single run would return the global optimum (in practice, this is true of all algorithms, under a finite budget of evaluations). The gold rule of optimization, regardless of optimization algorithm, is to always rerun the optimization multiple times from different starting points (a *multi-start* strategy), to explore the landscape of the target and gain some confidence about the results.\n",
    "\n",
    "Below, we rerun PyBADS `num_opts` times from different starting points and store the results of each run, which we will examine later. A few observations:\n",
    "\n",
    "- Each optimization uses a different `BADS` object (the general rule is: one BADS instance per optimization).\n",
    "- We specified `x0 = None`. This choice will randomly draw a starting point `x0` uniformly inside the provided plausible box, delimited by `plausible_lower_bounds` and `plausible_upper_bounds`.\n",
    "- For each run, we set a different, fixed random seed via `options['seed']`, which can be helpful for reproducibility of the results.\n",
    "- We switched off PyBADS default printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853bf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization 0...\n",
      "Running optimization 1...\n",
      "Running optimization 2...\n",
      "Running optimization 3...\n",
      "Running optimization 4...\n",
      "Running optimization 5...\n",
      "Running optimization 6...\n",
      "Running optimization 7...\n",
      "Running optimization 8...\n",
      "Running optimization 9...\n"
     ]
    }
   ],
   "source": [
    "num_opts = 10\n",
    "optimize_results = []\n",
    "x_vec = np.zeros((num_opts,lower_bounds.shape[0]))\n",
    "fval_vec = np.zeros(num_opts)\n",
    "\n",
    "for opt_count in range(num_opts):\n",
    "    print('Running optimization ' + str(opt_count) + '...')\n",
    "    options['rng_seed'] = opt_count\n",
    "    bads = BADS(\n",
    "        camelback6, None, lower_bounds, upper_bounds, plausible_lower_bounds, plausible_upper_bounds, options=options\n",
    "    )\n",
    "    optimize_results.append(bads.optimize())\n",
    "    x_vec[opt_count] = optimize_results[opt_count].x\n",
    "    fval_vec[opt_count] = optimize_results[opt_count].fval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7af7d8",
   "metadata": {},
   "source": [
    "## 3. Results and conclusions\n",
    "\n",
    "First, we inspect the results. In this example, the target function (six-hump camelback function) has two equally-good solutions:\n",
    "$$\n",
    "x^\\star = \\left\\{ (0.0898, -0.7126), (-0.0898, 0.7126) \\right\\}, \\qquad f(x^\\star) = -1.0316\n",
    "$$\n",
    "which should be represented in the set of results. \n",
    "\n",
    "Importantly, we should find below that (almost) all solutions are very close in function value, suggesting that we found the minimizers of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7af9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solutions:\n",
      "[[-0.09043313  0.71291211]\n",
      " [-0.09020921  0.71284823]\n",
      " [-0.08954086  0.71247826]\n",
      " [ 0.09022329 -0.71243114]\n",
      " [-0.09059099  0.711976  ]\n",
      " [ 0.09030418 -0.71285515]\n",
      " [ 0.09037972 -0.7129682 ]\n",
      " [ 0.09038288 -0.711995  ]\n",
      " [-0.08974587  0.71293963]\n",
      " [ 0.08949592 -0.71255345]]\n",
      "Function values at solutions:\n",
      "[-1.03162671 -1.0316277  -1.03162789 -1.03162739 -1.03162197 -1.03162739\n",
      " -1.0316267  -1.03162338 -1.03162773 -1.03162794]\n"
     ]
    }
   ],
   "source": [
    "print('Found solutions:')\n",
    "print(x_vec)\n",
    "\n",
    "print('Function values at solutions:')\n",
    "print(fval_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9834d1d4",
   "metadata": {},
   "source": [
    "We now take the best result of the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6507b31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BADS minimum at x_min = [ 0.08949592 -0.71255345]\n",
      "Function value at minimum fval = -1.0316279353323878\n"
     ]
    }
   ],
   "source": [
    "idx_best = np.argmin(fval_vec)\n",
    "result_best = optimize_results[idx_best]\n",
    "\n",
    "x_min = result_best['x']\n",
    "fval = result_best['fval']\n",
    "\n",
    "print(f\"BADS minimum at x_min = {x_min.flatten()}\")\n",
    "print(f\"Function value at minimum fval = {fval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ace561",
   "metadata": {},
   "source": [
    "The best result indeed matches $f^\\star = -1.0316$.\n",
    "\n",
    "The `OptimizeResult` object returned by PyBADS contains further information about the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eebe659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fun': <function __main__.camelback6(x)>,\n",
       " 'non_box_cons': None,\n",
       " 'target_type': 'deterministic',\n",
       " 'problem_type': 'bound constraints',\n",
       " 'iterations': 10,\n",
       " 'func_count': 79,\n",
       " 'mesh_size': 0.0009765625,\n",
       " 'overhead': 435.54597117720925,\n",
       " 'algorithm': 'Bayesian adaptive direct search',\n",
       " 'yval_vec': None,\n",
       " 'ysd_vec': None,\n",
       " 'x0': array([[-0.57853683, -1.11356753]]),\n",
       " 'x': array([ 0.08949592, -0.71255345]),\n",
       " 'fval': -1.0316279353323878,\n",
       " 'fsd': 0,\n",
       " 'total_time': 1.3011465082631837,\n",
       " 'random_seed': 9,\n",
       " 'version': '0.8.3.dev18+g0652341.d20230607',\n",
       " 'success': True,\n",
       " 'message': 'Optimization terminated: change in the function value less than options.tol_fun.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772490b5",
   "metadata": {},
   "source": [
    "In particular:\n",
    "\n",
    "- `total_time` is the total runtime (in seconds), including both the time spent evaluating the target and the algorithmic cost of BADS.\n",
    "- `overhead` represents the *fractional overhead* of BADS compared to the time spent evaluating the target. In this example, `overhead` is astronomical because the target function we are using is analytical and extremely fast, which is not what BADS is designed for. In a realistic scenario, the objective function will be moderately costly (e.g., more than 0.1 s per function evaluation), and the fractional overhead should be less than 1.\n",
    "- `seed` is the random seed used for this run (`None` if not specified)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f12204c93c4274de084c6b76e73171147c8e51a8507bf20dfb1db4f14f6829f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
