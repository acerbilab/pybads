[AdvancedOptions]
# Show optimization plots ("profile", "scatter", or "False")
plot = False

# Location of the global minimum (for debug only)
trueminx = []

# Tolerance and termination conditions
# Tolerance on mesh size
tolmesh = 1e-6
# Min significant change of objective fcn
tolfun = 1e-3
# Max iterations with no significant change (doubled under uncertainty)
tolstalliters = int(4 + np.floor(D/2))
# Min variabitility for a fcn to be considered noisy
tolnoise = np.spacing(1.0) * self.get("tolfun")

#Initialization
# Initialization function
initfcn = "init_sobol"
# Number of restarts attempts
restarts = 0
# Size of cache for storing fcn evaluations
cachesize = 500
# Number of initial objective fcn evaluations
ninit = D
# Pregress fcn evaluation with Y and X fields
funvalues = {}
funevalstart = 0 #np.maximum(D, 10)            # Number of initial target fcn evals

# Poll Options
pollmethod              = 'poll_mads_2n'     # Poll function
nbasis                  = 200 * D
pollmeshmultiplier      = 2.0                     # Mesh multiplicative factor between iterations
forcepollmesh           = False                 # Force poll vectors to be on mesh  
maxpollgridnumber       = 0                     # Maximum poll integer
alternativeincumbent    = False                 # Use alternative incumbent offset'
adaptiveincumbentshift  = False                 # Adaptive multiplier to incumbent uncertainty'
gprescalepoll           = 1.0                   # GP-based geometric scaling factor of poll vectors'
tolpoi                  = 1e-6/D                # Threshold probability of improvement (PoI); set to 0 to always complete polling'
skippoll                = True                  # Skip polling if PoI below threshold, even with no success'
consecutiveskipping     = True                  # Allow consecutive incomplete polls'
skippollaftersearch     = True                  # Skip polling after successful search'
minfailedpollsteps      = np.Inf                # Number of failed fcn evaluations before skipping is allowed'
acceleratemeshsteps     = 3                     # Accelerate mesh after this number of stalled iterations'
sloppyimprovement       = True                  # Move incumbent even after insufficient improvement'
meshoverflowswarning    = 2 + D/2                # Threshold # mesh overflows for warning'; 



# Improvement parameters
tolimprovement          = 1                    # Minimum significant improvement at unit mesh size'
forcingexponent         = 3/2                  # Exponent of forcing function'
incumbentsigmamultiplier = 0.1                 # Multiplier to incumbent uncertainty for acquisition functions'
improvementquantile     = 0.5                  # Quantile when computing improvement (<0.5 for conservative improvement)'
finalquantile           = 1e-3                 # Top quantile when choosing final iteration'

# Search properties
nsearch                 = 2**12                 # Number of candidate search points'
nsearchiter             = 2                    # Number of optimization iterations for search'
esbeta                  = 1                    # Multiplier in ES'
esstart                 = 0.25                 # Starting scale value in ES'
searchimprovefrac       = 0                    # Fraction of candidate search points with (slower) improved estimate'
searchscalesuccess      = np.sqrt(2)           # Search radius expansion factor for successful search'
searchscaleincremental  = 2                    # Search radius expansion factor for incremental search'
searchscalefailure      = np.sqrt(0.5)         # Search radius contraction factor for failed search'
searchfactormin         = 0.5
searchmethod            = [('ES-wcm',1), ('ES-ell',1)]  # Search function(s)'
searchgridnumber        = 10                   # Iteration scale factor between poll and search'
searchgridmultiplier    = 2                    # Multiplier integer scale factor between poll and search'
searchsizelocked       = True                  # Relative search scale factor locked to poll scale factor'
searchntry              = np.maximum(D, np.floor(3 + D/2)) # Number of searches per iteration'
searchmeshexpand        = 0                    # Search-triggered mesh expansion after this number of successful search rounds'
searchmeshincrement     = 1                    # Mesh size increment after search-triggered mesh expansion'
searchoptimize          = False                # Further optimize acquisition function'

# Noise parameters
uncertainincumbent      = True                  # Treat incumbent as if uncertain regardless of uncertainty handling'
meshnoisemultiplier      = 0.5                  # Contribution to log noise magnitude from log mesh size (0 for noisy functions)'

# Gaussian process properties
ndata                   = 50 + 10*D         # Number of training data (minimum 200 under uncertainty)
minndata                = 50                    # Minimum number of training data (doubled under uncertainty)
bufferndata             = 100                   # Max number of training data removed if too far from current point
gpsamples               = 0                     # Hyperparameters samples (0 = optimize)
gphypsampler = "slicesample"                    # MCMC sampler for GP hyperparameters
hpdfrac = 0.8                                   # High Posterior Density region (fraction of training inputs)
covsamplethresh = 10                            # Switch to covariance sampling below this threshold of stability index
gpsamplewidths = 0                              # Multiplier to widths from previous posterior for GP sampling (Inf = do not use previous widths)
weightedhypcov=True                             # Use weighted hyperparameter posterior covariance
tolcovweight = 0                                # Minimum weight for weighted hyperparameter posterior covariance
hyprunweight = 1                                # Weight of previous trials (per trial) for running avg of GP hyperparameter covariance
minrefittime            = 2*D               # Minimum fcn evals before refitting the GP
polltraining            = True                   # Train GP also during poll stage
doublerefit             = False                   # Always try a second GP fit
gp_meanfun              = 'const'
gp_covfun               = 1
gpmeanpercentile        = 90                    # Percentile of empirical GP mean
gpmeanrangefun          = lambda ym,y: (ym - np.median(y))/5*2   # Empirical range of hyperprior over the mean'    

gpdeffcn                = ('gp_def_bads','rq',[1,1])  # GP definition fcn'
gpmethod                = 'nearest'             # GP training set selection method'
gpcluster               = False                 # Cluster additional points during training
rotategp                = False                 # Rotate GP basis
gpradius                = 3                     # Radius of training set
useeffectiveradius      = True                  #
gpcovprior              = 'iso'                  # GP hyper-prior over covariance'
gpfixedmean             = False
fitlik                  = True                  # Fit the likelihood term
pollacqfcn              = ('acq_LCB', [])         # Acquisition fcn for poll stage
searchacqfcn            = ('acq_LCB', [])         # Acquisition fcn for search stage
acqhedge                = False                  # Hedge acquisition function
cholattempts            = 0                    # Attempts at performing the Cholesky decomposition
noisenudge              = np.array([1, 0])                # Increase nudge to noise in case of Cholesky failure
removepointsaftertries  = 1                    # Start removing training points after this number of failures
gpsvditers             = 200                  # SVGD iterations for GP training
gpwarnings              = False          # Issue warning if GP hyperparameters fit fails
normalphalevel          = 1e-6                 # Alpha level for normality test of gp predictions

gpsamplethin = 5                            # Thinning for GP hyperparameter sampling
stablegpsampling = 200 + 10 * D             # Force stable GP hyperparameter sampling (reduce samples or start optimizing)
gptrainninit = 1024                         # Initial design points for GP hyperparameter training
gptrainninitfinal = 64                      # Final design points for GP hyperparameter training
gptraininitmethod = "rand"                  # Initial design method for GP hyperparameter training
gptolopt = 1e-5                             # Tolerance for optimization of GP hyperparameters
gptoloptmcmc = 1e-2                         # Tolerance for optimization of GP hyperparameters preliminary to MCMC
nsgpmax = 0                                 # Max GP hyperparameter samples (decreases with training points)
nsgpmaxwarmup = 8                           # Max GP hyperparameter samples during warmup
nsgpmaxmain = np.Inf                        # Max GP hyperparameter samples during main algorithm
stablegpsamples = 0                         # Number of GP samples when GP is stable (0 = optimize)

gptoloptactive = 1e-4                       # Tolerance for optimization of GP hyperparameters during active sampling
gptoloptmcmcactive = 1e-2                   # Tolerance for optimization of GP hyperparameters preliminary to MCMC during active sampling
tolgpvar = 1e-4                             # Threshold True GP variance used by regulatized acquisition fcns
tolgpvarmcmc = 1e-4                         # Threshold True GP variance used to stabilize sampling
gpmeanfun = "const"                         # GP mean function
activesamplegpupdate = False                # Perform GP training after each active sample
sampleextravpmeans = 0                      # Extra variational components sampled from GP profile
integrategpmean = False                     # Try integrating GP mean function
tolsd = 0.1                                 # Tolerance True ELBO uncertainty for stopping (if variational posterior is stable)
tolskl = 0.01 * np.sqrt(D)                  # Stopping threshold True change of variational posterior per training point
tolstablewarmup = 15                        # Number of stable fcn evals for stopping warmup
variationalsampler = "malasample"           # MCMC sampler for variational posteriors
klgauss = True                              # Use Gaussian approximation for symmetrized KL-divergence b\w iters
kwarmup = 2                                 # Variational components during warmup
stablegpvpk = np.Inf                        # Force stable GP hyperparameter sampling after reaching this number of components
warpfunc = 0                                # GP warping function type



# Adaptive basis (unsupported)
hessianupdate           = False                 # Update Hessian as you go
hessianmethod           = 'bfgs'               # Hessian update method
hessianalternate        = False                 # Alternate Hessian iterations

# Hedge heuristic parameters (currently used during the search stage)
hedgegamma              = 0.125
hedgebeta               = 1e-3/self.get('tolfun')
hedgedecay              = 0.1**(1/(2*D))

# Max number of consecutive repeated measurements for noisy inputs
maxrepeatedobservations = 0
# Multiplicative discount True acquisition fcn to repeat measurement at the same location
repeatedacqdiscount = 1
# Base step size for stochastic gradient descent
sgdstepsize = 0.005
# Use ranking criterion to pick best non-converged solution
rankcriterion = True
# Run in diagnostics mode get additional info
diagnostics = False
# Output function
outputfcn = None

# Evaluated fcn values at X0
fvals = []
# Weighted proposal fcn for uncertainty search
proposalfcn = None
# Samples for fast acquisition fcn eval per new point
nssearch = 2 ** 13


# Set stochastic optimization stepsize via GP hyperparameters
gpstochasticstepsize = False
# True mean of the target density (for debugging)
truemean = []
# True covariance of the target density (for debugging)
truecov = []
# Min number of fcn evals
minfunevals = 5 * D
# Min number of iterations
miniter = D
# Fraction of search points from heavy-tailed variational posterior
heavytailsearchfrac = 0.25
# Fraction of search points from multivariate normal
mvnsearchfrac = 0.25
# Fraction of search points from multivariate normal fitted to HPD points
hpdsearchfrac = 0
# Fraction of search points from uniform random box based True training inputs
boxsearchfrac = 0.25
# Fraction of search points from previous iterations
searchcachefrac = 0
# Empirical Bayes prior over some GP hyperparameters
empiricalgpprior = False
# Minimum GP observation noise
tolgpnoise = np.sqrt(1e-5)
# Prior mean over GP input length scale (in plausible units)
gplengthpriormean = np.sqrt(D / 6)
# Prior std over GP input length scale (in plausible units)
gplengthpriorstd = 0.5 * np.log(1e3)
# Upper bound True GP input lengths based True plausible box (0 = ignore)
uppergplengthfactor = 0
# Initial samples (plausible is uniform in the plausible box)
initdesign = "plausible"
# Stricter upper bound True GP negative quadratic mean function
gpquadraticmeanbound = True
# Bandwidth parameter for GP smoothing (in units of plausible box)
bandwidth = 0
# Heuristic output warping (fitness shaping)
fitnessshaping = False
# Output warping starting threshold
outwarpthreshbase = 10 * D
# Output warping threshold multiplier when failed sub-threshold check
outwarpthreshmult = 1.25
# Output warping base threshold tolerance (fraction of current threshold)
outwarpthreshtol = 0.8
# Temperature for posterior tempering (allowed values T = 1234)
temperature = 1
# Use separate GP with constant mean for active search
separatesearchgp = False
# Discount observations from extremely low-density regions
noiseshaping = False
# Threshold from max observed value to start discounting
noiseshapingthreshold = 10 * D
# Proportionality factor of added noise wrt distance from threshold
noiseshapingfactor = 0.05
# Past iterations window to judge acquisition fcn improvement
acqhedgeiterwindow = 4
# Portfolio value decay per function evaluation
acqhedgedecay = 0.9

# Active search bound multiplier
activesearchbound = 2
# Tolerance True closeness to bound constraints (fraction of total range)
tolboundx = 1e-5
# Recompute LCB max for each iteration based True current GP estimate
recomputelcbmax = True
# Input transform for bounded variables
boundedtransform = "logit"
# Use double GP
doublegp = False
# Warp every this number of iterations
warpeveryiters = 5
# Increase delay between warpings
incrementalwarpdelay = True
# Threshold True reliability index to perform warp
warptolreliability = 3
# Rotate and scale input
warprotoscaling = True
# Regularization weight towards diagonal covariance matrix for N training inputs
warpcovreg = 0
# Threshold True correlation matrix for roto-scaling
warprotocorrthresh = 0.05