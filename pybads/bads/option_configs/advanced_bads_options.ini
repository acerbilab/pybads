[AdvancedOptions]
# Show optimization plots ("profile", "scatter", or "False")
plot = False

# Tolerance and termination conditions
# Tolerance on mesh size
tolmesh = 1e-6
# Min significant change of objective fcn
tolfun = 1e-3
# Max iterations with no significant change (doubled under uncertainty)
tolstalliters = int(4 + np.floor(D/2))
# Min variabitility for a fcn to be considered noisy
tolnoise = np.spacing(1.0) * self.get("tolfun")

#Initialization
# Initialization function
initfcn = "init_sobol"
# Number of restarts attempts
restarts = 0
# Size of cache for storing fcn evaluations
cachesize = 500
# Number of initial objective fcn evaluations
fun_eval_start = D
# Pregress fcn evaluation with Y and X fields
funvalues = {}
# Array with indices of periodic variables, like periodic_vars = [1, 2]
periodic_vars = None

# Poll Options
# Poll function
pollmethod                              = 'poll_mads_2n'        

nbasis                                  = 200 * D
# Mesh multiplicative factor between iterations
pollmeshmultiplier                      = 2.0                   
# Force poll vectors to be on mesh
forcepollmesh                           = False                 
# Maximum poll integer
maxpollgridnumber                       = 0                     
# Use alternative incumbent offset'
alternativeincumbent                    = False                 
# Adaptive multiplier to incumbent uncertainty'
adaptiveincumbentshift                  = False                 
# GP-based geometric scaling factor of poll vectors'
gprescalepoll                           = 1.0                   
# Threshold probability of improvement (PoI); set to 0 to always complete polling'
tolpoi                                  = 1e-6/D                
# Skip polling if PoI below threshold, even with no success'
skippoll                                = True                  
# Allow consecutive incomplete polls'
consecutiveskipping                     = True                  
# Skip polling after successful search'
skippollaftersearch                     = True                  
# Number of failed fcn evaluations before skipping is allowed'
minfailedpollsteps                      = np.Inf                
# Accelerate mesh after this number of stalled iterations'
accelerate_mesh_steps                     = 3                  
# Move incumbent even after insufficient improvement
sloppyimprovement                       = True                  
# Move incumbent even for the uncertain unsuccess when Sto-BADS is configured
opp_stobads                             = True                  
# Power value of the Sto-BADS incumbent decision rule:  \gamma *  \epsilon * frame_size**(power_value)
stobads_frame_size_scaling_power        = 2                     
# Threshold # mesh overflows for warning';
meshoverflowswarning                    = 2 + D/2               
# Initial mesh size (power value)
init_mesh_size_integer                  = 0                     

# StoBADS option, if True switch to stochastic optimization and uncertain incumbent
stobads = False

# Improvement parameters
# Minimum significant improvement at unit mesh size'
tolimprovement          = 1                    
# Exponent of forcing function'
forcingexponent         = 3/2                  
# Multiplier to incumbent uncertainty for acquisition functions'
incumbentsigmamultiplier = 0.1                 
# Quantile when computing improvement (<0.5 for conservative improvement)'
improvementquantile     = 0.5                  
# Top quantile when choosing final iteration'
finalquantile           = 1e-3                 

# Search properties
# Number of candidate search points'
nsearch                 = 2**12                
# Number of optimization iterations for search'
nsearchiter             = 2                    
# Multiplier in ES'
esbeta                  = 1                    
# Starting scale value in ES'
esstart                 = 0.25                 
# Fraction of candidate search points with (slower) improved estimate'
searchimprovefrac       = 0                    
# Search radius expansion factor for successful search'
searchscalesuccess      = np.sqrt(2)           
# Search radius expansion factor for incremental search'
searchscaleincremental  = 2                    
# Search radius contraction factor for failed search'
searchscalefailure      = np.sqrt(0.5)         
searchfactormin         = 0.5
# Search function(s) (list of tuples with function name and sumrule flag)'
searchmethod            = [('ES-wcm',1), ('ES-ell',1)]  
# Iteration scale factor between poll and search'
searchgridnumber        = 10                   
# Multiplier integer scale factor between poll and search'
searchgridmultiplier    = 2                    
# Relative search scale factor locked to poll scale factor'
searchsizelocked       = True                  
# Number of searches per iteration'
searchntry              = np.maximum(D, np.floor(3 + D/2)) 
# Search-triggered mesh expansion after this number of successful search rounds'
searchmeshexpand        = 0                    
# Mesh size increment after search-triggered mesh expansion'
searchmeshincrement     = 1                    
# Further optimize acquisition function'
searchoptimize          = False                

# Noise parameters
# Treat incumbent as if uncertain regardless of uncertainty handling'
uncertainincumbent      = True                  
# Contribution to log noise magnitude from log mesh size (0 for noisy functions)'
meshnoisemultiplier      = 0.5                  

# Gaussian process properties
# Number of training data (minimum 200 under uncertainty)
ntrain_max                = 50 + 10*D           
# Minimum number of training data (doubled under uncertainty)
ntrain_min                = 50                  
# Max number of training data removed if too far from current point
buffer_ntrain             = 100                 
# Hyperparameters samples (0 = optimize)
gpsamples                 = 0                     
# MCMC sampler for GP hyperparameters
gphypsampler = "slicesample"                    
# High Posterior Density region (fraction of training inputs)
hpd_frac = 0.8                                  
# Switch to covariance sampling below this threshold of stability index
covsamplethresh = 10                            
# Multiplier to widths from previous posterior for GP sampling (Inf = do not use previous widths)
gpsamplewidths = 0                              
# Use weighted hyperparameter posterior covariance
weightedhypcov=True                             
# Minimum weight for weighted hyperparameter posterior covariance
tolcovweight = 0                                
# Weight of previous trials (per trial) for running avg of GP hyperparameter covariance
hyprunweight = 1                                
# Minimum fcn evals before refitting the GP
minrefittime            = 2*D                   
# Train GP also during poll stage
polltraining            = True                  
# Always try a second GP fit
doublerefit             = False  
# GP mean function               
gp_meanfun              = 'const'
gp_covfun               = 1
# Percentile of empirical GP mean
gp_mean_percentile        = 90                    
# Empirical range of hyperprior over the mean
gp_mean_range_fun          = lambda ym,y: (ym - np.median(y))/5*2   
# GP definition fcn'
gpdeffcn                = ('gp_def_bads','rq',[1,1])  
# GP training set selection method'
gpmethod                = 'nearest'             
# Cluster additional points during training
gpcluster               = False                 
# Rotate GP basis
rotategp                = False                 
# Radius of training set
gpradius                = 3                     
use_effective_radius      = True                
# GP hyper-prior over covariance'
gpcovprior              = 'iso'                  
gp_fixed_mean             = False
# Fit the likelihood term
fitlik                  = True                  
# Acquisition fcn for poll stage
pollacqfcn              = ('acq_LCB', None)         
# Acquisition fcn for search stage
searchacqfcn            = ('acq_LCB', None)         
# Hedge acquisition function
acqhedge                = False                  
# Attempts at performing the Cholesky decomposition
cholattempts            = 0                    
# Increase nudge to noise in case of Cholesky failure
noise_nudge             = np.array([1, 0])                
# Start removing training points after this number of failures
remove_points_after_tries  = 1                    
# SVGD iterations for GP training
gpsvditers              = 200                  
# Issue warning if GP hyperparameters fit fails
gp_warnings             = False          
# Alpha level for normality test of gp predictions
normalphalevel          = 1e-6                 
# Number of target fcn evals per iteration
fun_evals_per_iter = 1                      
# Thinning for GP hyperparameter sampling
gpsamplethin = 5                            
# Force stable GP hyperparameter sampling (reduce samples or start optimizing)
stablegpsampling = 200 + 10 * D             
# Initial design points for GP hyperparameter training
gp_train_n_init = 128                         
# Final design points for GP hyperparameter training
gp_train_n_init_final = 8                      
# Initial design method for GP hyperparameter training
gptraininitmethod = "rand"                  
# Tolerance for optimization of GP hyperparameters
gptolopt = 1e-5                             
# Tolerance for optimization of GP hyperparameters preliminary to MCMC
gptoloptmcmc = 1e-2                         
# Max GP hyperparameter samples (decreases with training points)
nsgpmax = 0                                 
# Max GP hyperparameter samples during warmup
nsgpmaxwarmup = 8                           
# Max GP hyperparameter samples during main algorithm
nsgpmaxmain = np.Inf                        
# Number of GP samples when GP is stable (0 = optimize)
stablegpsamples = 0                         
# Tolerance for optimization of GP hyperparameters during active sampling
gptoloptactive = 1e-4                       
# Tolerance for optimization of GP hyperparameters preliminary to MCMC during active sampling
gptoloptmcmcactive = 1e-2                   
# Threshold True GP variance used by regulatized acquisition fcns
tolgpvar = 1e-4                             
# Threshold True GP variance used to stabilize sampling
tolgpvarmcmc = 1e-4                        
# Perform GP training after each active sample
activesamplegpupdate = False                
# Extra variational components sampled from GP profile
sampleextravpmeans = 0                      
# Try integrating GP mean function
integrategpmean = False                     
# Tolerance True ELBO uncertainty for stopping (if variational posterior is stable)
tolsd = 0.1                                 
# Stopping threshold True change of variational posterior per training point
tolskl = 0.01 * np.sqrt(D)                  
# Number of stable fcn evals for stopping warmup
tolstablewarmup = 15                        
# MCMC sampler for variational posteriors
variationalsampler = "malasample"           
# Use Gaussian approximation for symmetrized KL-divergence b\w iters
klgauss = True                              
# Variational components during warmup
kwarmup = 2                                 
# Force stable GP hyperparameter sampling after reaching this number of components
stablegpvpk = np.Inf                        
# GP warping function type
warpfunc = 0                                
# Slice sampler option for prior hyper-parameter sampling method
use_slice_sampler = False                   

# Adaptive basis (unsupported)
hessianupdate           = False                 # Update Hessian as you go
hessianmethod           = 'bfgs'               # Hessian update method
hessianalternate        = False                 # Alternate Hessian iterations

# Hedge heuristic parameters (currently used during the search stage)
hedgegamma              = 0.125
hedgebeta               = 1e-3/self.get('tolfun')
hedgedecay              = 0.1**(1/(2*D))

# Max number of consecutive repeated measurements for noisy inputs
maxrepeatedobservations = 0
# Multiplicative discount True acquisition fcn to repeat measurement at the same location
repeatedacqdiscount = 1
# Base step size for stochastic gradient descent
sgdstepsize = 0.005
# Use ranking criterion to pick best non-converged solution
rankcriterion = True
# Run in diagnostics mode get additional info
diagnostics = False
# Output function
outputfcn = None

# Evaluated fcn values at X0
fvals = None
# Samples for fast acquisition fcn eval per new point
nssearch = 2 ** 13
# Set stochastic optimization stepsize via GP hyperparameters
gpstochasticstepsize = False
# Min number of fcn evals
minfunevals = 5 * D
# Min number of iterations
miniter = D
# Fraction of search points from heavy-tailed variational posterior
heavytailsearchfrac = 0.25
# Fraction of search points from multivariate normal
mvnsearchfrac = 0.25
# Fraction of search points from multivariate normal fitted to HPD points
hpdsearchfrac = 0
# Fraction of search points from uniform random box based True training inputs
boxsearchfrac = 0.25
# Fraction of search points from previous iterations
searchcachefrac = 0
# Empirical Bayes prior over some GP hyperparameters
empiricalgpprior = False
# Minimum GP observation noise
tolgpnoise = np.sqrt(1e-5)
# Prior mean over GP input length scale (in plausible units)
gplengthpriormean = np.sqrt(D / 6)
# Prior std over GP input length scale (in plausible units)
gplengthpriorstd = 0.5 * np.log(1e3)
# Upper bound True GP input lengths based True plausible box (0 = ignore)
uppergplengthfactor = 0
# Initial samples (plausible is uniform in the plausible box)
initdesign = "plausible"
# Stricter upper bound True GP negative quadratic mean function
gpquadraticmeanbound = True
# Bandwidth parameter for GP smoothing (in units of plausible box)
bandwidth = 0
# Heuristic output warping (fitness shaping)
fitness_shaping = False
# Output warping starting threshold
outwarpthreshbase = 10 * D
# Output warping threshold multiplier when failed sub-threshold check
outwarpthreshmult = 1.25
# Output warping base threshold tolerance (fraction of current threshold)
outwarpthreshtol = 0.8
# Temperature for posterior tempering (allowed values T = 1234)
temperature = 1
# Use separate GP with constant mean for active search
separatesearchgp = False
# Discount observations from extremely low-density regions
noiseshaping = False
# Threshold from max observed value to start discounting
noiseshapingthreshold = 10 * D
# Proportionality factor of added noise wrt distance from threshold
noiseshapingfactor = 0.05
# Past iterations window to judge acquisition fcn improvement
acqhedgeiterwindow = 4
# Portfolio value decay per function evaluation
acqhedgedecay = 0.9

# Active search bound multiplier
activesearchbound = 2
# Tolerance True closeness to bound constraints (fraction of total range)
tolboundx = 1e-5
# Recompute LCB max for each iteration based True current GP estimate
recomputelcbmax = True
# Use double GP
doublegp = False
# Warp every this number of iterations
warpeveryiters = 5
# Increase delay between warpings
incrementalwarpdelay = True
# Threshold True reliability index to perform warp
warptolreliability = 3
# Rotate and scale input
warprotoscaling = True
# Regularization weight towards diagonal covariance matrix for N training inputs
warpcovreg = 0
# Threshold True correlation matrix for roto-scaling
warprotocorrthresh = 0.05
